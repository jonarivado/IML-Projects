{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error\n",
    "import parfit.parfit as pf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set\n",
    "df_train_features = pd.read_csv('train_features.csv')\n",
    "df_train_labels = pd.read_csv('train_labels.csv')\n",
    "df_test_features = pd.read_csv('test_features.csv')\n",
    "\n",
    "# add labels with matching pid to features\n",
    "labels_copied = pd.DataFrame()\n",
    "labels_copied = df_train_labels.loc[df_train_labels.index.repeat(12)]\n",
    "labels_copied = labels_copied.drop(columns=['pid'])\n",
    "labels = df_train_labels.drop(columns=['pid'])\n",
    "\n",
    "LABELS1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "LABELS2 = ['LABEL_Sepsis']\n",
    "LABELS3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "\n",
    "# impute the missing data on the test set\n",
    "TEST_X = df_test_features.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(df_test_features.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation set and impute missing data\n",
    "X = df_train_features.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(df_train_features.mean())\n",
    "y_1 = labels_copied[LABELS1]\n",
    "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X, y_1, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a random forest classifier with every hour for every patient\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "for i, label in enumerate(LABELS1):\n",
    "    clf.fit(X_train_1, y_train_1[label])\n",
    "    y_pred_proba = clf.predict_proba(X_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35541453 0.52504998 0.547615   0.5507502  0.54549137 0.49915421\n",
      " 0.64785733 0.48366763 0.59570544 0.88577012]\n",
      "Average score:  0.5636475819306217\n"
     ]
    }
   ],
   "source": [
    "err = np.empty(10)\n",
    "list_proba = list()\n",
    "for i,label in enumerate(LABELS1):\n",
    "    err[i] = roc_auc_score(y_val_1[label], y_pred_proba[:,1])\n",
    "    list_proba.append(y_pred_proba[:,1])\n",
    "print(err)\n",
    "print('Average score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4702586443092772\n",
      "0.5164655075800315\n",
      "0.49806408127351276\n",
      "0.49789758509082677\n",
      "0.49446048672532833\n",
      "0.5264744185072168\n",
      "0.5232551449086882\n",
      "0.5015396393331065\n",
      "0.5175795372378886\n",
      "0.6083089895100252\n",
      "Average Score:  0.5154304034475902\n"
     ]
    }
   ],
   "source": [
    "err = np.empty(10)\n",
    "for i, labels in enumerate(LABELS1):\n",
    "    y_val_reduced = y_val_1[labels][0:len(y_val_1[labels]):12]\n",
    "    y_pred_proba_reduced = np.empty(int(len(y_pred_proba[:,1])/12))\n",
    "    counter = 0\n",
    "    for splits in np.split(np.array(y_pred_proba[:,1]), int(len(y_pred_proba[:,1])/12)):\n",
    "        y_pred_proba_reduced[counter] = splits.mean() \n",
    "        counter = counter+1\n",
    "    err[i] = roc_auc_score(y_val_reduced, y_pred_proba_reduced)\n",
    "    print(err[i])\n",
    "print('Average Score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba = clf.predict_proba(TEST_X)\n",
    "\n",
    "TEST_list_proba = list()\n",
    "for j in range(10):\n",
    "    TEST_y_pred_proba_reduced = np.empty(int(len(TEST_y_pred_proba[j][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_proba[j][:,1]), int(len(TEST_y_pred_proba[j][:,1])/12)):\n",
    "        TEST_y_pred_proba_reduced[counter] = splits.mean() \n",
    "        counter = counter+1\n",
    "    TEST_list_proba.append(TEST_y_pred_proba_reduced)\n",
    "proba_subtask1 = TEST_list_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation set and impute missing data\n",
    "y_2 = labels_copied[LABELS2]\n",
    "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X, y_2, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a random forest classifier with every hour for every patient\n",
    "clf = RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train_2, y_train_2)\n",
    "y_pred_proba = clf.predict_proba(X_val_2)\n",
    "err = roc_auc_score(y_val_2, y_pred_proba[:,1])\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba = clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba_reduced = np.empty(int(len(TEST_y_pred_proba[:,1])/12))\n",
    "counter = 0\n",
    "for splits in np.split(np.array(TEST_y_pred_proba[:,1]), int(len(TEST_y_pred_proba[:,1])/12)):\n",
    "    TEST_y_pred_proba_reduced[counter] = splits.mean() \n",
    "    counter = counter+1\n",
    "proba_subtask2 = TEST_y_pred_proba_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation set and impute missing data\n",
    "y_3 = labels_copied[LABELS3]\n",
    "X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(X, y_3, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "model = RidgeCV(alphas=(0.1, 1, 10), cv=cv)\n",
    "model.fit(X_train_3, y_train_3)\n",
    "y_pred = model.predict(X_val_3)\n",
    "print(mean_squared_error(y_val_3, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, labels in enumerate(LABELS3):\n",
    "    y_test_reduced = y_val_3[labels][0:len(y_val_3[labels]):12]\n",
    "    y_pred_reduced = np.empty(int(len(y_pred[:,i])/12))\n",
    "    counter = 0\n",
    "    for splits in np.split(np.array(y_pred[:,i]), int(len(y_pred[:,i])/12)):\n",
    "        y_pred_reduced[counter] = splits.mean() \n",
    "        counter = counter+1\n",
    "    print(mean_squared_error(y_test_reduced, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_values = model.predict(TEST_X)\n",
    "print(TEST_y_pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_values = list()\n",
    "for j in range(4):\n",
    "    TEST_y_pred_values_reduced = np.empty(int(len(TEST_y_pred_values[:,j])/12))\n",
    "    counter = 0\n",
    "    for splits in np.split(np.array(TEST_y_pred_values[:,j]), int(len(TEST_y_pred_values[:,j])/12)):\n",
    "        TEST_y_pred_values_reduced[counter] = splits.mean() \n",
    "        counter = counter+1\n",
    "    TEST_list_values.append(TEST_y_pred_values_reduced)\n",
    "proba_subtask3=TEST_list_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample.zip'\n",
    "df_submission = pd.read_csv(filename)\n",
    "for i,label in enumerate(LABELS1):\n",
    "    # round classification labels\n",
    "    df_submission[label] = proba_subtask1[i]\n",
    "df_submission[LABELS2[0]] = proba_subtask2\n",
    "for i,label in enumerate(LABELS3):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask3[i]\n",
    "df_submission.to_csv('submission.csv',index=False)\n",
    "df_submission.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7df4d15e59cac213afef75c805c6c35b948ade230879836dc77c436b6318e246"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('intro-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
