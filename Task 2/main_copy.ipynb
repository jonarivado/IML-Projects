{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data=pd.read_csv('train_features.csv')\n",
    "train_labels_data=pd.read_csv('train_labels.csv')\n",
    "test_features_data=pd.read_csv('test_features.csv')\n",
    "# add labels with matching pid to features\n",
    "labels_copied = pd.DataFrame()\n",
    "labels_copied = train_labels_data.loc[train_labels_data.index.repeat(12)]\n",
    "labels_copied = labels_copied.drop(columns=['pid'])\n",
    "labels = train_labels_data.drop(columns=['pid'])\n",
    "LABELS1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "LABELS2 = ['LABEL_Sepsis']\n",
    "LABELS3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())\n",
    "y = labels_copied[LABELS1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "TEST_X = test_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83805999, 0.74313657, 0.71377258, 0.71558142, 0.71343182,\n",
       "       0.75722364, 0.81129012, 0.78336141, 0.78300249, 0.91290222])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.777176226325439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=np.empty(10)\n",
    "list_proba=list()\n",
    "for i,label in enumerate(LABELS1):\n",
    "    err[i]=roc_auc_score(y_test[label], y_pred_proba[i][:,1])\n",
    "    list_proba.append(y_pred_proba[i][:,1])\n",
    "display(err)\n",
    "display('Average score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5927854807202311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5990483441187666"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5667199468119817"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5677165524379841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5734232537970856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5764141299655984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5848663213038594"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5806972914949041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6778188822078667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7057045852880367"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average Score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6025194788146315"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err=np.empty(10)\n",
    "for i, labels in enumerate(LABELS1):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_proba_reduced=np.empty(int(len(y_pred_proba[i][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred_proba[i][:,1]), int(len(y_pred_proba[i][:,1])/12)):\n",
    "        y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    err[i]=roc_auc_score(y_test_reduced, y_pred_proba_reduced)\n",
    "    display(err[i])\n",
    "display('Average Score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_proba=list()\n",
    "for j in range(10):\n",
    "    TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[j][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_proba[j][:,1]), int(len(TEST_y_pred_proba[j][:,1])/12)):\n",
    "        TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_proba.append(TEST_y_pred_proba_reduced)\n",
    "proba_subtask1=TEST_list_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidschwartz/opt/anaconda3/envs/intro-ml/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7531643474813929"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[:,1])/12))\n",
    "counter=0\n",
    "for splits in np.split(np.array(TEST_y_pred_proba[:,1]), int(len(TEST_y_pred_proba[:,1])/12)):\n",
    "    TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "    counter=counter+1\n",
    "proba_subtask2=TEST_y_pred_proba_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sub-Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.1472790830782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "model = RidgeCV(alphas=(0.1, 1, 10), cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.178231632838566"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "158.76495455996073"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.081079608443603"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "210.5812593355625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, labels in enumerate(LABELS3):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_reduced=np.empty(int(len(y_pred[:,i])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred[:,i]), int(len(y_pred[:,i])/12)):\n",
    "        y_pred_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    display(mean_squared_error(y_test_reduced, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.42979709, 84.63189056, 97.16603678, 85.22237716],\n",
       "       [16.44438825, 81.04771302, 97.90423776, 80.56801682],\n",
       "       [16.39009215, 83.05933386, 98.02093075, 82.87670078],\n",
       "       ...,\n",
       "       [18.14305129, 77.00011097, 97.72903206, 87.64944768],\n",
       "       [18.44245049, 79.01844308, 97.73463876, 87.83224154],\n",
       "       [17.45013289, 77.12264536, 97.74997756, 86.53530667]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEST_y_pred_values=model.predict(TEST_X)\n",
    "display(TEST_y_pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_values=list()\n",
    "for j in range(4):\n",
    "    TEST_y_pred_values_reduced=np.empty(int(len(TEST_y_pred_values[:,j])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_values[:,j]), int(len(TEST_y_pred_values[:,j])/12)):\n",
    "        TEST_y_pred_values_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_values.append(TEST_y_pred_values_reduced)\n",
    "proba_subtask3=TEST_list_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample.zip'\n",
    "df_submission = pd.read_csv(filename)\n",
    "for i,label in enumerate(LABELS1):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask1[i]\n",
    "df_submission[LABELS2[0]]=proba_subtask2\n",
    "for i,label in enumerate(LABELS3):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask3[i]\n",
    "df_submission.to_csv('submission.csv',index=False)\n",
    "df_submission.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7df4d15e59cac213afef75c805c6c35b948ade230879836dc77c436b6318e246"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('intro-ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
