{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data=pd.read_csv('train_features.csv')\n",
    "train_labels_data=pd.read_csv('train_labels.csv')\n",
    "test_features_data=pd.read_csv('test_features.csv')\n",
    "# add labels with matching pid to features\n",
    "labels_copied = pd.DataFrame()\n",
    "labels_copied = train_labels_data.loc[train_labels_data.index.repeat(12)]\n",
    "labels_copied = labels_copied.drop(columns=['pid'])\n",
    "labels = train_labels_data.drop(columns=['pid'])\n",
    "LABELS1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "LABELS2 = ['LABEL_Sepsis']\n",
    "LABELS3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())\n",
    "y = labels_copied[LABELS1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, shuffle=True)\n",
    "\n",
    "TEST_X = test_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest model and Roc_Auc_Score using every hour per patient as sample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84238713, 0.75417446, 0.71588294, 0.71788092, 0.71524152,\n",
       "       0.7659438 , 0.81023819, 0.78541174, 0.78157756, 0.90859597])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.779733425206673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1 )\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=np.empty(10)\n",
    "list_proba=list()\n",
    "for i,label in enumerate(LABELS1):\n",
    "    err[i]=roc_auc_score(y_test[label], y_pred_proba[i][:,1])\n",
    "    list_proba.append(y_pred_proba[i][:,1])\n",
    "display(err)\n",
    "display('Average score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for test-labels and y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.632890365448505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5982732351447436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5375155666251556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5236486486486487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5408954279386173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6079314608726374"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6486866791744841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.37945945945945947"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6469155844155844"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average Score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5691216427727837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err=np.empty(10)\n",
    "for i, labels in enumerate(LABELS1):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_proba_reduced=np.empty(int(len(y_pred_proba[i][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred_proba[i][:,1]), int(len(y_pred_proba[i][:,1])/12)):\n",
    "        y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    err[i]=roc_auc_score(y_test_reduced, y_pred_proba_reduced)\n",
    "    display(err[i])\n",
    "display('Average Score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_proba=list()\n",
    "for j in range(10):\n",
    "    TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[j][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_proba[j][:,1]), int(len(TEST_y_pred_proba[j][:,1])/12)):\n",
    "        TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_proba.append(TEST_y_pred_proba_reduced)\n",
    "proba_subtask1=TEST_list_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest model and Roc_Auc_Score using every hour per patient as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manue\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7120814987481655"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[:,1])/12))\n",
    "counter=0\n",
    "for splits in np.split(np.array(TEST_y_pred_proba[:,1]), int(len(TEST_y_pred_proba[:,1])/12)):\n",
    "    TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "    counter=counter+1\n",
    "proba_subtask2=TEST_y_pred_proba_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement MultiTaskLassoCV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.78080988076304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "model = RidgeCV(alphas=(0.1, 1, 10), cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for test-labels and y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.31047510321523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "151.526961808906"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.808054191742678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "268.7465567434238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, labels in enumerate(LABELS3):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_reduced=np.empty(int(len(y_pred[:,i])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred[:,i]), int(len(y_pred[:,i])/12)):\n",
    "        y_pred_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    display(mean_squared_error(y_test_reduced, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.42989954, 84.66569541, 97.16850264, 85.20450256],\n",
       "       [16.52744139, 80.99853638, 98.08966975, 80.68784629],\n",
       "       [16.36000022, 83.1671906 , 98.04754418, 83.01852774],\n",
       "       ...,\n",
       "       [18.14832684, 77.00301936, 97.73806462, 87.6635914 ],\n",
       "       [18.44623374, 79.02377686, 97.74498834, 87.84807596],\n",
       "       [17.45422415, 77.13011665, 97.75621869, 86.55120625]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEST_y_pred_values=model.predict(TEST_X)\n",
    "display(TEST_y_pred_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_values=list()\n",
    "for j in range(4):\n",
    "    TEST_y_pred_values_reduced=np.empty(int(len(TEST_y_pred_values[:,j])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_values[:,j]), int(len(TEST_y_pred_values[:,j])/12)):\n",
    "        TEST_y_pred_values_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_values.append(TEST_y_pred_values_reduced)\n",
    "proba_subtask3=TEST_list_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write values into sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample.zip'\n",
    "df_submission = pd.read_csv(filename)\n",
    "for i,label in enumerate(LABELS1):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask1[i]\n",
    "df_submission[LABELS2[0]]=proba_subtask2\n",
    "for i,label in enumerate(LABELS3):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask3[i]\n",
    "df_submission.to_csv('submission.csv',index=False)\n",
    "df_submission.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d6ccad5fb65be54beb4ea4c1483fd63b24b72db810c67b5f7bf1972f6225f55"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
