{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_data=pd.read_csv('train_features.csv')\n",
    "train_labels_data=pd.read_csv('train_labels.csv')\n",
    "test_features_data=pd.read_csv('test_features.csv')\n",
    "# add labels with matching pid to features\n",
    "labels_copied = pd.DataFrame()\n",
    "labels_copied = train_labels_data.loc[train_labels_data.index.repeat(12)]\n",
    "labels_copied = labels_copied.drop(columns=['pid'])\n",
    "labels = train_labels_data.drop(columns=['pid'])\n",
    "LABELS1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "LABELS2 = ['LABEL_Sepsis']\n",
    "LABELS3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())\n",
    "y = labels_copied[LABELS1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, shuffle=True, random_state=42)\n",
    "\n",
    "TEST_X = test_features_data.drop(columns=['pid', 'Time']).reset_index(drop=True).fillna(test_features_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest model and Roc_Auc_Score using every hour per patient as sample for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84444609, 0.74624438, 0.7218315 , 0.72349666, 0.71586261,\n",
       "       0.76938312, 0.81472183, 0.80230192, 0.78972144, 0.91998747])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7847997017963831"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1 )\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=np.empty(10)\n",
    "list_proba=list()\n",
    "for i,label in enumerate(LABELS1):\n",
    "    err[i]=roc_auc_score(y_test[label], y_pred_proba[i][:,1])\n",
    "    list_proba.append(y_pred_proba[i][:,1])\n",
    "display(err)\n",
    "display('Average score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for test-labels and y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5973368070632509"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5763888888888888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6168573943661972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6093016431924883"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6217542270531401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5546757164404224"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6191904047976013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5534909909909911"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7298956414978515"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7138798701298701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Average Score: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6192771584420702"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err=np.empty(10)\n",
    "for i, labels in enumerate(LABELS1):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_proba_reduced=np.empty(int(len(y_pred_proba[i][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred_proba[i][:,1]), int(len(y_pred_proba[i][:,1])/12)):\n",
    "        y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    err[i]=roc_auc_score(y_test_reduced, y_pred_proba_reduced)\n",
    "    display(err[i])\n",
    "display('Average Score: ', np.mean(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_proba=list()\n",
    "for j in range(10):\n",
    "    TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[j][:,1])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_proba[j][:,1]), int(len(TEST_y_pred_proba[j][:,1])/12)):\n",
    "        TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_proba.append(TEST_y_pred_proba_reduced)\n",
    "proba_subtask1=TEST_list_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest model and Roc_Auc_Score using every hour per patient as sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manue\\anaconda3\\envs\\ML\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7263875992984627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=300, class_weight=None, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba=clf.predict_proba(X_test)\n",
    "err=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "display(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba=clf.predict_proba(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_y_pred_proba_reduced=np.empty(int(len(TEST_y_pred_proba[:,1])/12))\n",
    "counter=0\n",
    "for splits in np.split(np.array(TEST_y_pred_proba[:,1]), int(len(TEST_y_pred_proba[:,1])/12)):\n",
    "    TEST_y_pred_proba_reduced[counter]=splits.mean() \n",
    "    counter=counter+1\n",
    "proba_subtask2=TEST_y_pred_proba_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_copied[LABELS3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement MultiTaskLassoCV model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.9649507805526\n",
      "0.29397840307348955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\n",
    "model = RidgeCV(alphas=(0.1, 1, 10), cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for test-labels and y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.286756647275718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "166.05384745478807"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.06387575457702"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "180.9739500447106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, labels in enumerate(LABELS3):\n",
    "    y_test_reduced=y_test[labels][0:len(y_test[labels]):12]\n",
    "    y_pred_reduced=np.empty(int(len(y_pred[:,i])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(y_pred[:,i]), int(len(y_pred[:,i])/12)):\n",
    "        y_pred_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    display(mean_squared_error(y_test_reduced, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for Test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.43060338, 84.67101428, 97.16760692, 85.19986706],\n",
       "       [16.51291492, 80.6929088 , 98.10689491, 80.5983686 ],\n",
       "       [16.37222118, 83.14702545, 98.04809191, 82.99084645],\n",
       "       ...,\n",
       "       [18.14853251, 77.00880845, 97.73806771, 87.66333252],\n",
       "       [18.44695555, 79.02849377, 97.74488846, 87.84801953],\n",
       "       [17.45441408, 77.13508889, 97.75643728, 86.5513912 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEST_y_pred_values=model.predict(TEST_X)\n",
    "display(TEST_y_pred_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce values by 12 for TEST_y_pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_list_values=list()\n",
    "for j in range(4):\n",
    "    TEST_y_pred_values_reduced=np.empty(int(len(TEST_y_pred_values[:,j])/12))\n",
    "    counter=0\n",
    "    for splits in np.split(np.array(TEST_y_pred_values[:,j]), int(len(TEST_y_pred_values[:,j])/12)):\n",
    "        TEST_y_pred_values_reduced[counter]=splits.mean() \n",
    "        counter=counter+1\n",
    "    TEST_list_values.append(TEST_y_pred_values_reduced)\n",
    "proba_subtask3=TEST_list_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write values into sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample.zip'\n",
    "df_submission = pd.read_csv(filename)\n",
    "for i,label in enumerate(LABELS1):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask1[i]\n",
    "df_submission[LABELS2[0]]=proba_subtask2\n",
    "for i,label in enumerate(LABELS3):\n",
    "    # round classification labels\n",
    "    df_submission[label]=proba_subtask3[i]\n",
    "df_submission.to_csv('submission.csv',index=False)\n",
    "df_submission.to_csv('prediction.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d6ccad5fb65be54beb4ea4c1483fd63b24b72db810c67b5f7bf1972f6225f55"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
